{"cells":[{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iK0WWW0LRCPr","executionInfo":{"status":"ok","timestamp":1680785091049,"user_tz":-480,"elapsed":1058,"user":{"displayName":"Jeromy Gladden","userId":"11403350924762830733"}},"outputId":"f74ac8e5-5e75-450c-c953-9ebbe0211b3f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Apr  6 12:44:49 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   61C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/LianjiaTech/BELLE.git "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u5X9zmRCRNer","executionInfo":{"status":"ok","timestamp":1680785103351,"user_tz":-480,"elapsed":2855,"user":{"displayName":"Jeromy Gladden","userId":"11403350924762830733"}},"outputId":"54b61664-b906-400a-e119-f594dcccb227"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'BELLE'...\n","remote: Enumerating objects: 665, done.\u001b[K\n","remote: Counting objects: 100% (468/468), done.\u001b[K\n","remote: Compressing objects: 100% (296/296), done.\u001b[K\n","remote: Total 665 (delta 328), reused 273 (delta 169), pack-reused 197\u001b[K\n","Receiving objects: 100% (665/665), 2.22 MiB | 2.43 MiB/s, done.\n","Resolving deltas: 100% (368/368), done.\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.utils.cpp_extension\n","print(torch.utils.cpp_extension.CUDA_HOME)\n","print(torch.__version__)\n","print(torch.version.cuda)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RFGyWBvnmWzn","executionInfo":{"status":"ok","timestamp":1680785898455,"user_tz":-480,"elapsed":564,"user":{"displayName":"Jeromy Gladden","userId":"11403350924762830733"}},"outputId":"d9e26ff5-3095-42c3-da4f-232f2ae34f72"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/cuda\n","2.0.0+cu118\n","11.8\n"]}]},{"cell_type":"code","source":["%cd BELLE/gptq"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X7aQTZ_dRQ3B","executionInfo":{"status":"ok","timestamp":1680785416172,"user_tz":-480,"elapsed":1054,"user":{"displayName":"Jeromy Gladden","userId":"11403350924762830733"}},"outputId":"8625c23e-eaf9-4f10-fe6f-31f2b7b283da"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/BELLE/gptq\n"]}]},{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-wYX26z8SsUS","executionInfo":{"status":"ok","timestamp":1680785471337,"user_tz":-480,"elapsed":52145,"user":{"displayName":"Jeromy Gladden","userId":"11403350924762830733"}},"outputId":"b52ea0a6-b9ac-47c6-8f70-9ac065d1270e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/huggingface/transformers (from -r requirements.txt (line 4))\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-_yizqlv8\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-_yizqlv8\n","  Resolved https://github.com/huggingface/transformers to commit 12d51db243a00726a548a43cc333390ebae731e3\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting safetensors==0.3.0\n","  Downloading safetensors-0.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets==2.10.1\n","  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess\n","  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiohttp\n","  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (2023.3.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (6.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (1.22.4)\n","Collecting dill<0.3.7,>=0.3.0\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting xxhash\n","  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (9.0.0)\n","Collecting huggingface-hub<1.0.0,>=0.2.0\n","  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (4.65.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (1.4.4)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (2.27.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0->-r requirements.txt (line 4)) (3.10.7)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0->-r requirements.txt (line 4)) (2022.10.31)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (22.2.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (2.0.12)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1->-r requirements.txt (line 2)) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (3.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 2)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 2)) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.10.1->-r requirements.txt (line 2)) (1.16.0)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.28.0.dev0-py3-none-any.whl size=6894883 sha256=336d0cf7be9ef562f668d28fe9315e46155e6beb350214f61abaf14fef49631c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-h662ezsv/wheels/14/a0/7b/8f6b25ba4110aa215fcb8d6aedd6cd4f9b9b6619190999ac2b\n","Successfully built transformers\n","Installing collected packages: tokenizers, sentencepiece, safetensors, xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.10.1 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.13.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 safetensors-0.3.0 sentencepiece-0.1.97 tokenizers-0.13.3 transformers-4.28.0.dev0 xxhash-3.2.0 yarl-1.8.2\n"]}]},{"cell_type":"code","source":["!pip uninstall -y  transformers\n","!pip install  git+https://github.com/huggingface/transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZMp_PaPDSxOq","executionInfo":{"status":"ok","timestamp":1680785512774,"user_tz":-480,"elapsed":41446,"user":{"displayName":"Jeromy Gladden","userId":"11403350924762830733"}},"outputId":"627d3005-451d-4fe1-db3f-31fc00c390d0"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: transformers 4.28.0.dev0\n","Uninstalling transformers-4.28.0.dev0:\n","  Successfully uninstalled transformers-4.28.0.dev0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-5ndcple4\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-5ndcple4\n","  Resolved https://github.com/huggingface/transformers to commit 12d51db243a00726a548a43cc333390ebae731e3\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (1.22.4)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (0.13.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (2.27.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (2022.10.31)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (0.13.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (4.65.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (3.10.7)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0.dev0) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (3.4)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.28.0.dev0-py3-none-any.whl size=6894883 sha256=f61a3fb9e7a4967bc43427239f97527d755e38f70b5d4248e2eb637c0c313a3c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-c8tdk513/wheels/14/a0/7b/8f6b25ba4110aa215fcb8d6aedd6cd4f9b9b6619190999ac2b\n","Successfully built transformers\n","Installing collected packages: transformers\n","Successfully installed transformers-4.28.0.dev0\n"]}]},{"cell_type":"code","source":["! python setup_cuda.py install && CUDA_VISIBLE_DEVICES=0 && python test_kernel.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pVULDg34S2PL","executionInfo":{"status":"ok","timestamp":1680785821880,"user_tz":-480,"elapsed":121051,"user":{"displayName":"Jeromy Gladden","userId":"11403350924762830733"}},"outputId":"b40f7210-7842-474d-9338-42eafb2ce43b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["running install\n","/usr/local/lib/python3.9/dist-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/setuptools/command/easy_install.py:144: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n","  warnings.warn(\n","running bdist_egg\n","running egg_info\n","creating quant_cuda.egg-info\n","writing quant_cuda.egg-info/PKG-INFO\n","writing dependency_links to quant_cuda.egg-info/dependency_links.txt\n","writing top-level names to quant_cuda.egg-info/top_level.txt\n","writing manifest file 'quant_cuda.egg-info/SOURCES.txt'\n","/usr/local/lib/python3.9/dist-packages/torch/utils/cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n","  warnings.warn(msg.format('we could not find ninja.'))\n","reading manifest file 'quant_cuda.egg-info/SOURCES.txt'\n","writing manifest file 'quant_cuda.egg-info/SOURCES.txt'\n","installing library code to build/bdist.linux-x86_64/egg\n","running install_lib\n","running build_ext\n","/usr/local/lib/python3.9/dist-packages/torch/utils/cpp_extension.py:398: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 11.8\n","  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n","building 'quant_cuda' extension\n","creating build\n","creating build/temp.linux-x86_64-3.9\n","x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.9/dist-packages/torch/include -I/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/dist-packages/torch/include/TH -I/usr/local/lib/python3.9/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.9 -c quant_cuda.cpp -o build/temp.linux-x86_64-3.9/quant_cuda.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=quant_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n","/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.9/dist-packages/torch/include -I/usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.9/dist-packages/torch/include/TH -I/usr/local/lib/python3.9/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.9 -c quant_cuda_kernel.cu -o build/temp.linux-x86_64-3.9/quant_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=quant_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n","\u001b[01m\u001b[0m\u001b[01m/usr/local/lib/python3.9/dist-packages/torch/include/c10/util/irange.h(54)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n","          detected during:\n","            instantiation of \u001b[01m\"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\u001b[0m \u001b[32m\n","(61): here\u001b[0m\n","            instantiation of \u001b[01m\"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=size_t, one_sided=false, <unnamed>=0]\"\u001b[0m \u001b[32m\n","/usr/local/lib/python3.9/dist-packages/torch/include/c10/core/TensorImpl.h(77): here\u001b[0m\n","\n","\u001b[01m\u001b[0m\u001b[01m/usr/local/lib/python3.9/dist-packages/torch/include/c10/util/irange.h(54)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n","          detected during:\n","            instantiation of \u001b[01m\"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator==(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\u001b[0m \u001b[32m\n","(61): here\u001b[0m\n","            instantiation of \u001b[01m\"__nv_bool c10::detail::integer_iterator<I, one_sided, <unnamed>>::operator!=(const c10::detail::integer_iterator<I, one_sided, <unnamed>> &) const [with I=std::size_t, one_sided=true, <unnamed>=0]\"\u001b[0m \u001b[32m\n","/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/qualified_name.h(73): here\u001b[0m\n","\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:123:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  123 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:222:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  222 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:123:163:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  123 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/Dispatch.h:122:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  122 | \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n","      | \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:123:163:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  123 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/Dispatch.h:122:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  122 | \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n","      | \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:123:1011:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  123 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:123:1032:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  123 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:123:1056:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  123 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:123:1083:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  123 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:123:1106:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  123 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:123:1999:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  123 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:123:2020:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  123 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:123:2043:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  123 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:123:2069:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  123 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:123:2092:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  123 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:218:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  218 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:222:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  222 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:218:163:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  218 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/Dispatch.h:122:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  122 | \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n","      | \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:218:163:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  218 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/Dispatch.h:122:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  122 | \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n","      | \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:218:1011:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  218 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:218:1032:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  218 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:218:1056:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  218 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:218:1083:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  218 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:218:1106:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  218 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:218:1999:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  218 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:218:2020:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  218 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:218:2043:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  218 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:218:2069:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  218 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:218:2092:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  218 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:377:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  377 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:222:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  222 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:377:163:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  377 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/Dispatch.h:122:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  122 | \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n","      | \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:377:163:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  377 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/Dispatch.h:122:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  122 | \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n","      | \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:377:1011:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  377 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:377:1032:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  377 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:377:1056:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  377 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:377:1083:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  377 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:377:1106:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  377 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:377:1999:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  377 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:377:2020:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  377 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:377:2043:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  377 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:377:2069:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  377 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:377:2092:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  377 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:464:41:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[K’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  464 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:222:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  222 | \u001b[01;36m\u001b[K  De\u001b[m\u001b[KprecatedTypeProperties & type() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:464:163:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  464 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/Dispatch.h:122:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  122 | \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n","      | \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:464:163:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  464 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/Dispatch.h:122:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  122 | \u001b[01;36m\u001b[Kinline at::\u001b[m\u001b[KScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n","      | \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:464:1011:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  464 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:464:1032:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  464 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:464:1056:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  464 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:464:1083:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = double]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  464 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:464:1106:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  464 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:\u001b[m\u001b[K In lambda function:\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:464:1999:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  464 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:464:2020:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  464 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:464:2043:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  464 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:464:2069:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = float]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  464 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kquant_cuda_kernel.cu:464:2092:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KT* at::Tensor::data() const [with T = int]\u001b[m\u001b[K’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n","  464 |   AT_DISPATCH_FLOATING_TYPES(\n","      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[K/usr/local/lib/python3.9/dist-packages/torch/include/ATen/core/TensorBody.h:244:1:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n","  244 | \u001b[01;36m\u001b[K  T \u001b[m\u001b[K* data() const {\n","      | \u001b[01;36m\u001b[K^\u001b[m\u001b[K \u001b[01;36m\u001b[K~~\u001b[m\u001b[K\n","creating build/lib.linux-x86_64-3.9\n","x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.9/quant_cuda.o build/temp.linux-x86_64-3.9/quant_cuda_kernel.o -L/usr/local/lib/python3.9/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.9/quant_cuda.cpython-39-x86_64-linux-gnu.so\n","creating build/bdist.linux-x86_64\n","creating build/bdist.linux-x86_64/egg\n","copying build/lib.linux-x86_64-3.9/quant_cuda.cpython-39-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n","creating stub loader for quant_cuda.cpython-39-x86_64-linux-gnu.so\n","byte-compiling build/bdist.linux-x86_64/egg/quant_cuda.py to quant_cuda.cpython-39.pyc\n","creating build/bdist.linux-x86_64/egg/EGG-INFO\n","copying quant_cuda.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying quant_cuda.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying quant_cuda.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying quant_cuda.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n","zip_safe flag not set; analyzing archive contents...\n","__pycache__.quant_cuda.cpython-39: module references __file__\n","creating dist\n","creating 'dist/quant_cuda-0.0.0-py3.9-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n","removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n","Processing quant_cuda-0.0.0-py3.9-linux-x86_64.egg\n","creating /usr/local/lib/python3.9/dist-packages/quant_cuda-0.0.0-py3.9-linux-x86_64.egg\n","Extracting quant_cuda-0.0.0-py3.9-linux-x86_64.egg to /usr/local/lib/python3.9/dist-packages\n","Adding quant-cuda 0.0.0 to easy-install.pth file\n","\n","Installed /usr/local/lib/python3.9/dist-packages/quant_cuda-0.0.0-py3.9-linux-x86_64.egg\n","Processing dependencies for quant-cuda==0.0.0\n","Finished processing dependencies for quant-cuda==0.0.0\n","Benchmarking LLaMa-7B FC2 matvec ...\n","FP16: 0.0006524114608764648\n","2bit: 0.001129288911819458\n","3bit: 0.0008859856128692627\n","4bit: 0.0008313274383544922\n","8bit: 0.0006593968868255615\n","Verifiying kernel correctness ...\n","2bit Simu: tensor([[[ 0.1773,  0.1909,  0.4069,  ..., -0.6696, -0.4636, -0.5461],\n","         [ 0.5607, -0.5629,  0.1498,  ..., -0.2001,  0.4124,  0.5971],\n","         [-1.1808, -0.1661,  0.2279,  ..., -0.9640, -0.4783,  0.0650],\n","         ...,\n","         [ 0.0256,  0.2193, -1.0445,  ..., -0.6018, -0.2980,  0.3822],\n","         [-0.5831, -0.9861, -0.7115,  ...,  0.8948, -0.0444, -0.1198],\n","         [ 0.4737, -0.7688,  1.0295,  ..., -0.7308, -0.1865, -0.5383]],\n","\n","        [[ 0.6848,  0.2999,  0.0303,  ...,  0.3859, -0.7303,  0.5425],\n","         [ 0.0682, -0.4488, -0.5079,  ..., -0.0443, -0.4046,  0.2441],\n","         [ 0.6428, -0.4309, -0.2512,  ..., -0.5010,  0.4581, -0.1283],\n","         ...,\n","         [ 0.5235, -0.3268, -0.5335,  ...,  0.4971, -0.0991,  0.0877],\n","         [-0.3975,  0.0761, -0.0881,  ..., -0.0839, -0.6629,  0.3110],\n","         [ 0.2224, -0.5245,  0.6823,  ..., -0.3808,  0.4868, -0.8549]],\n","\n","        [[-0.9551,  0.2034, -0.0888,  ...,  0.8756, -0.8087,  0.1512],\n","         [ 0.0179,  0.0515, -0.0868,  ...,  0.5004, -0.6204,  0.3881],\n","         [-0.4733,  0.3593, -1.1677,  ...,  0.1423,  0.4179,  0.5684],\n","         ...,\n","         [-0.0401, -0.6069,  0.4233,  ..., -0.4700, -0.7006,  0.8708],\n","         [-0.2808,  1.0864,  0.7612,  ..., -0.3076, -0.3790,  0.8840],\n","         [-0.4554,  0.2830, -0.0127,  ..., -1.1597,  0.5407, -0.0851]],\n","\n","        [[ 0.0254, -0.5394, -0.4589,  ...,  0.0200,  0.0332,  0.5923],\n","         [ 0.3938, -1.0987,  1.0250,  ..., -0.7220, -0.2307, -0.7653],\n","         [-0.7040,  0.2355,  0.8718,  ..., -0.2500,  1.4254,  0.5584],\n","         ...,\n","         [-0.2440, -0.3149, -0.2932,  ...,  0.5363, -0.1940,  0.1550],\n","         [-0.4222, -0.3957,  0.0432,  ..., -0.5175,  0.1194,  0.1504],\n","         [ 0.7598,  0.0580,  0.8042,  ..., -0.2860, -0.6977, -0.0885]],\n","\n","        [[ 0.1960,  0.4106, -0.0475,  ...,  0.2125,  0.4573,  0.8303],\n","         [ 0.7159,  0.1131, -0.0270,  ..., -0.1274,  0.4227,  0.1151],\n","         [-0.2130,  0.2856, -0.0145,  ..., -0.7169,  0.1371,  0.1904],\n","         ...,\n","         [-0.7761,  1.3790,  0.4113,  ...,  0.0347,  0.2010,  0.0529],\n","         [ 0.3625,  0.4042, -0.2880,  ...,  0.4410, -0.8270, -0.5859],\n","         [ 1.1065, -0.1211,  0.6418,  ..., -0.0054,  0.7665, -0.7807]]],\n","       device='cuda:0')\n","2bit Kern: tensor([[[ 0.1773,  0.1909,  0.4069,  ..., -0.6696, -0.4636, -0.5461],\n","         [ 0.5607, -0.5629,  0.1498,  ..., -0.2001,  0.4124,  0.5971],\n","         [-1.1808, -0.1661,  0.2279,  ..., -0.9640, -0.4783,  0.0650],\n","         ...,\n","         [ 0.0256,  0.2193, -1.0445,  ..., -0.6018, -0.2980,  0.3822],\n","         [-0.5831, -0.9861, -0.7115,  ...,  0.8948, -0.0444, -0.1198],\n","         [ 0.4737, -0.7688,  1.0295,  ..., -0.7308, -0.1865, -0.5383]],\n","\n","        [[ 0.6848,  0.2999,  0.0303,  ...,  0.3859, -0.7303,  0.5425],\n","         [ 0.0682, -0.4488, -0.5079,  ..., -0.0443, -0.4046,  0.2441],\n","         [ 0.6428, -0.4309, -0.2512,  ..., -0.5010,  0.4581, -0.1283],\n","         ...,\n","         [ 0.5235, -0.3268, -0.5335,  ...,  0.4971, -0.0991,  0.0877],\n","         [-0.3975,  0.0761, -0.0881,  ..., -0.0839, -0.6629,  0.3110],\n","         [ 0.2224, -0.5245,  0.6823,  ..., -0.3808,  0.4868, -0.8549]],\n","\n","        [[-0.9551,  0.2034, -0.0888,  ...,  0.8756, -0.8087,  0.1512],\n","         [ 0.0179,  0.0515, -0.0868,  ...,  0.5004, -0.6204,  0.3881],\n","         [-0.4733,  0.3593, -1.1677,  ...,  0.1423,  0.4179,  0.5684],\n","         ...,\n","         [-0.0401, -0.6069,  0.4233,  ..., -0.4700, -0.7006,  0.8708],\n","         [-0.2808,  1.0864,  0.7612,  ..., -0.3076, -0.3790,  0.8840],\n","         [-0.4554,  0.2830, -0.0127,  ..., -1.1597,  0.5407, -0.0851]],\n","\n","        [[ 0.0254, -0.5394, -0.4589,  ...,  0.0200,  0.0332,  0.5923],\n","         [ 0.3938, -1.0987,  1.0250,  ..., -0.7220, -0.2307, -0.7653],\n","         [-0.7040,  0.2355,  0.8718,  ..., -0.2500,  1.4254,  0.5584],\n","         ...,\n","         [-0.2440, -0.3149, -0.2932,  ...,  0.5363, -0.1940,  0.1550],\n","         [-0.4222, -0.3957,  0.0432,  ..., -0.5175,  0.1194,  0.1504],\n","         [ 0.7598,  0.0580,  0.8042,  ..., -0.2860, -0.6977, -0.0885]],\n","\n","        [[ 0.1960,  0.4106, -0.0475,  ...,  0.2125,  0.4573,  0.8303],\n","         [ 0.7159,  0.1131, -0.0270,  ..., -0.1274,  0.4227,  0.1151],\n","         [-0.2130,  0.2856, -0.0145,  ..., -0.7169,  0.1371,  0.1904],\n","         ...,\n","         [-0.7761,  1.3790,  0.4113,  ...,  0.0347,  0.2010,  0.0529],\n","         [ 0.3625,  0.4042, -0.2880,  ...,  0.4410, -0.8270, -0.5859],\n","         [ 1.1065, -0.1211,  0.6418,  ..., -0.0054,  0.7665, -0.7807]]],\n","       device='cuda:0')\n","\n","\n","3bit Simu: tensor([[[ 9.4708e-01, -4.2603e-01,  7.3661e-01,  ...,  1.0588e-01,\n","           2.4947e-01,  5.1812e-01],\n","         [-5.7028e-01, -5.4108e-01,  4.4797e-01,  ..., -1.4724e-02,\n","           5.6639e-01,  3.4823e-02],\n","         [-8.8401e-02,  4.5438e-01, -2.1105e+00,  ..., -1.5024e-01,\n","          -7.8282e-02,  5.9703e-01],\n","         ...,\n","         [-1.6785e-01, -1.4410e-01,  3.6382e-01,  ...,  4.5379e-01,\n","           6.5482e-01, -1.0812e-01],\n","         [-9.6763e-01,  2.6534e-01,  9.5172e-02,  ..., -1.2752e-01,\n","           1.0066e-01,  3.7039e-01],\n","         [-7.9792e-03, -7.0708e-02, -3.3209e-01,  ..., -5.0062e-01,\n","           6.2709e-01,  7.4924e-02]],\n","\n","        [[ 1.5519e-01, -6.1078e-02, -4.3918e-01,  ..., -1.7478e-01,\n","          -5.6035e-02,  6.3986e-01],\n","         [ 3.6725e-02, -3.5965e-01, -3.4471e-02,  ..., -4.6746e-01,\n","          -3.9747e-01, -5.3709e-01],\n","         [ 8.3676e-02, -2.5731e-01, -8.9884e-02,  ..., -4.1390e-01,\n","           2.2820e-01,  3.9193e-01],\n","         ...,\n","         [ 9.3983e-02,  5.4416e-02, -9.5431e-02,  ..., -9.0105e-01,\n","          -1.0544e+00, -5.1615e-02],\n","         [ 1.0963e+00,  1.3814e+00, -5.7168e-01,  ...,  1.5344e-01,\n","          -4.4103e-01, -3.4026e-01],\n","         [-4.2490e-01,  1.1095e-01,  6.2974e-01,  ...,  2.9151e-01,\n","           1.0488e+00, -4.2565e-01]],\n","\n","        [[ 5.7794e-01,  6.9114e-02, -1.1662e+00,  ...,  3.9822e-02,\n","           8.5816e-01, -5.7318e-02],\n","         [-3.9298e-01, -1.6937e+00,  1.2595e+00,  ...,  4.8174e-01,\n","           2.4424e-01,  7.0610e-02],\n","         [-4.2044e-01,  1.5162e-01, -3.4282e-01,  ...,  8.0937e-01,\n","           3.3836e-01, -5.9421e-01],\n","         ...,\n","         [ 4.5652e-02, -3.5187e-01, -9.5397e-01,  ..., -5.4673e-01,\n","           9.8336e-04, -4.5658e-01],\n","         [ 3.7493e-01, -5.1590e-01, -1.5884e-02,  ..., -5.9912e-01,\n","          -7.6494e-01, -5.9184e-01],\n","         [ 5.0372e-01,  4.3341e-01, -3.0092e-01,  ..., -3.4863e-01,\n","           5.0775e-01,  4.5227e-01]],\n","\n","        [[ 1.0850e-01, -1.8185e-01,  6.3468e-01,  ...,  4.7475e-01,\n","           5.7734e-01, -5.8127e-01],\n","         [ 4.3936e-01,  5.5568e-02, -1.3245e+00,  ...,  2.9318e-01,\n","           3.0582e-02, -5.7104e-01],\n","         [-7.5587e-01, -6.4671e-02,  2.9874e-01,  ...,  1.7643e-01,\n","           8.9313e-01, -4.0501e-01],\n","         ...,\n","         [-3.5322e-01,  3.2230e-02,  2.0584e-01,  ...,  2.7730e-01,\n","          -5.8449e-01, -1.0369e-01],\n","         [-2.9874e-02, -3.0862e-01,  4.2597e-02,  ...,  1.3063e-01,\n","           8.2818e-02, -5.1027e-01],\n","         [ 4.4383e-01, -6.1971e-01, -8.5069e-02,  ...,  1.0397e+00,\n","          -8.0822e-01,  1.5006e-01]],\n","\n","        [[ 8.1819e-01, -6.5445e-01, -1.6977e-01,  ..., -4.9920e-01,\n","           2.4656e-01, -4.2604e-01],\n","         [ 2.7928e-01,  1.9871e-01, -7.6998e-02,  ...,  1.3454e+00,\n","          -6.4731e-01, -9.2163e-02],\n","         [ 6.3565e-01,  1.1019e+00,  1.2615e+00,  ...,  4.6102e-01,\n","           2.4035e-01, -1.1798e+00],\n","         ...,\n","         [ 1.7637e-01,  2.8051e-01,  2.0246e-01,  ...,  2.3438e-01,\n","           4.2801e-01, -2.5817e-01],\n","         [-1.0626e+00,  1.3329e+00,  4.4513e-02,  ..., -5.3652e-01,\n","           1.0430e+00,  6.0942e-01],\n","         [ 1.4976e+00, -1.9869e-01,  8.7824e-01,  ..., -1.7639e-01,\n","           6.0344e-01,  7.1512e-02]]], device='cuda:0')\n","3bit Kern: tensor([[[ 9.4708e-01, -4.2602e-01,  7.3661e-01,  ...,  1.0588e-01,\n","           2.4947e-01,  5.1812e-01],\n","         [-5.7028e-01, -5.4108e-01,  4.4797e-01,  ..., -1.4724e-02,\n","           5.6639e-01,  3.4823e-02],\n","         [-8.8401e-02,  4.5438e-01, -2.1105e+00,  ..., -1.5024e-01,\n","          -7.8282e-02,  5.9703e-01],\n","         ...,\n","         [-1.6785e-01, -1.4410e-01,  3.6382e-01,  ...,  4.5379e-01,\n","           6.5483e-01, -1.0812e-01],\n","         [-9.6762e-01,  2.6535e-01,  9.5172e-02,  ..., -1.2751e-01,\n","           1.0066e-01,  3.7039e-01],\n","         [-7.9791e-03, -7.0708e-02, -3.3209e-01,  ..., -5.0062e-01,\n","           6.2709e-01,  7.4923e-02]],\n","\n","        [[ 1.5519e-01, -6.1078e-02, -4.3918e-01,  ..., -1.7478e-01,\n","          -5.6035e-02,  6.3986e-01],\n","         [ 3.6725e-02, -3.5965e-01, -3.4471e-02,  ..., -4.6746e-01,\n","          -3.9747e-01, -5.3709e-01],\n","         [ 8.3676e-02, -2.5731e-01, -8.9884e-02,  ..., -4.1390e-01,\n","           2.2820e-01,  3.9193e-01],\n","         ...,\n","         [ 9.3982e-02,  5.4417e-02, -9.5431e-02,  ..., -9.0105e-01,\n","          -1.0544e+00, -5.1615e-02],\n","         [ 1.0963e+00,  1.3814e+00, -5.7168e-01,  ...,  1.5344e-01,\n","          -4.4103e-01, -3.4026e-01],\n","         [-4.2490e-01,  1.1095e-01,  6.2974e-01,  ...,  2.9151e-01,\n","           1.0488e+00, -4.2565e-01]],\n","\n","        [[ 5.7794e-01,  6.9114e-02, -1.1662e+00,  ...,  3.9822e-02,\n","           8.5816e-01, -5.7318e-02],\n","         [-3.9298e-01, -1.6937e+00,  1.2595e+00,  ...,  4.8174e-01,\n","           2.4424e-01,  7.0611e-02],\n","         [-4.2044e-01,  1.5162e-01, -3.4282e-01,  ...,  8.0937e-01,\n","           3.3836e-01, -5.9422e-01],\n","         ...,\n","         [ 4.5652e-02, -3.5187e-01, -9.5397e-01,  ..., -5.4673e-01,\n","           9.8326e-04, -4.5658e-01],\n","         [ 3.7493e-01, -5.1590e-01, -1.5884e-02,  ..., -5.9912e-01,\n","          -7.6494e-01, -5.9184e-01],\n","         [ 5.0372e-01,  4.3341e-01, -3.0092e-01,  ..., -3.4863e-01,\n","           5.0775e-01,  4.5227e-01]],\n","\n","        [[ 1.0850e-01, -1.8185e-01,  6.3468e-01,  ...,  4.7475e-01,\n","           5.7734e-01, -5.8127e-01],\n","         [ 4.3936e-01,  5.5569e-02, -1.3245e+00,  ...,  2.9319e-01,\n","           3.0582e-02, -5.7104e-01],\n","         [-7.5587e-01, -6.4671e-02,  2.9874e-01,  ...,  1.7643e-01,\n","           8.9313e-01, -4.0501e-01],\n","         ...,\n","         [-3.5322e-01,  3.2230e-02,  2.0584e-01,  ...,  2.7730e-01,\n","          -5.8449e-01, -1.0369e-01],\n","         [-2.9874e-02, -3.0862e-01,  4.2597e-02,  ...,  1.3063e-01,\n","           8.2818e-02, -5.1027e-01],\n","         [ 4.4383e-01, -6.1971e-01, -8.5069e-02,  ...,  1.0397e+00,\n","          -8.0822e-01,  1.5006e-01]],\n","\n","        [[ 8.1819e-01, -6.5445e-01, -1.6977e-01,  ..., -4.9920e-01,\n","           2.4656e-01, -4.2604e-01],\n","         [ 2.7928e-01,  1.9871e-01, -7.6998e-02,  ...,  1.3454e+00,\n","          -6.4731e-01, -9.2163e-02],\n","         [ 6.3565e-01,  1.1019e+00,  1.2615e+00,  ...,  4.6102e-01,\n","           2.4035e-01, -1.1798e+00],\n","         ...,\n","         [ 1.7637e-01,  2.8051e-01,  2.0246e-01,  ...,  2.3438e-01,\n","           4.2801e-01, -2.5817e-01],\n","         [-1.0626e+00,  1.3329e+00,  4.4514e-02,  ..., -5.3652e-01,\n","           1.0430e+00,  6.0942e-01],\n","         [ 1.4976e+00, -1.9869e-01,  8.7824e-01,  ..., -1.7639e-01,\n","           6.0343e-01,  7.1513e-02]]], device='cuda:0')\n","\n","\n","4bit Simu: tensor([[[ 0.9584, -0.2810,  0.4793,  ...,  0.8630, -0.0840,  0.2375],\n","         [-0.2626,  1.1432, -0.4041,  ...,  0.0148, -0.1495,  0.0028],\n","         [-0.9595, -0.3090, -0.7770,  ..., -0.0411,  1.1869, -0.9239],\n","         ...,\n","         [ 0.2906, -0.6580, -0.9088,  ...,  0.6305,  0.8486, -0.1609],\n","         [-0.5215, -0.5117, -0.5382,  ...,  0.2307,  1.2033, -0.1436],\n","         [ 0.3269, -0.5769,  0.6719,  ..., -0.2787,  0.0492, -0.6816]],\n","\n","        [[ 0.0671, -0.3195, -0.1774,  ...,  0.7549, -0.4655,  0.3576],\n","         [ 0.2742, -0.2666, -0.3088,  ..., -0.7936, -0.1744, -0.0133],\n","         [-0.1225,  0.3283,  0.5311,  ..., -0.4299, -0.6863, -0.3983],\n","         ...,\n","         [ 0.5403,  0.1997,  0.1856,  ..., -0.4454,  0.3453, -0.2628],\n","         [ 0.3871, -0.9231, -0.1666,  ...,  0.0772, -0.0444,  0.2590],\n","         [-0.0812,  1.4779,  0.8988,  ..., -1.1146,  1.0448,  1.0519]],\n","\n","        [[-0.4362,  0.5773, -0.0062,  ...,  0.9260, -0.1116,  0.4853],\n","         [-0.3355,  0.3770,  0.7200,  ...,  0.0410,  0.2072,  0.0545],\n","         [ 0.0357,  0.1501, -0.6396,  ...,  1.1995, -0.4276,  1.2270],\n","         ...,\n","         [-0.4789,  0.1443,  1.0770,  ...,  0.3193,  0.3184,  0.3362],\n","         [ 0.0622, -0.4742, -0.3382,  ...,  0.8847, -0.1844, -0.0771],\n","         [-0.2194,  0.2591, -0.0956,  ...,  0.3737, -0.1478, -0.2293]],\n","\n","        [[-0.7696,  0.1531, -0.8572,  ..., -0.3858,  0.0534, -0.6434],\n","         [-0.2538,  1.1371,  0.0241,  ..., -0.5719,  0.6910, -0.7004],\n","         [ 1.3673, -0.7895, -0.8872,  ...,  0.2463,  0.0526,  0.5488],\n","         ...,\n","         [ 0.7119, -0.1724,  0.1081,  ..., -0.8122,  0.1016, -0.5708],\n","         [-0.1348,  0.2648,  0.7257,  ..., -0.2262,  0.3651,  0.5955],\n","         [ 0.4330,  0.9720,  0.3746,  ...,  0.4828, -0.4878, -0.0786]],\n","\n","        [[-0.3675,  0.0417,  0.0444,  ..., -0.3165, -0.9441,  0.6468],\n","         [-0.0139,  0.4126,  0.7862,  ...,  0.1945,  0.5425,  0.2139],\n","         [ 0.5893,  0.0330,  1.0036,  ..., -0.0628, -0.2772,  0.7403],\n","         ...,\n","         [ 0.8017,  0.5026, -0.2135,  ...,  0.1897,  0.8163, -0.8402],\n","         [-0.1256,  0.1596, -0.2899,  ...,  0.4217, -0.2899,  0.0264],\n","         [-0.1038, -0.0115, -0.2936,  ...,  0.8732, -0.8528, -0.5979]]],\n","       device='cuda:0')\n","4bit Kern: tensor([[[ 0.9584, -0.2810,  0.4793,  ...,  0.8630, -0.0840,  0.2375],\n","         [-0.2626,  1.1432, -0.4041,  ...,  0.0148, -0.1495,  0.0028],\n","         [-0.9595, -0.3090, -0.7770,  ..., -0.0411,  1.1869, -0.9239],\n","         ...,\n","         [ 0.2906, -0.6580, -0.9088,  ...,  0.6305,  0.8486, -0.1609],\n","         [-0.5215, -0.5117, -0.5382,  ...,  0.2307,  1.2033, -0.1436],\n","         [ 0.3269, -0.5769,  0.6719,  ..., -0.2787,  0.0492, -0.6816]],\n","\n","        [[ 0.0671, -0.3195, -0.1774,  ...,  0.7549, -0.4655,  0.3576],\n","         [ 0.2742, -0.2666, -0.3088,  ..., -0.7936, -0.1744, -0.0133],\n","         [-0.1225,  0.3283,  0.5311,  ..., -0.4299, -0.6863, -0.3983],\n","         ...,\n","         [ 0.5403,  0.1997,  0.1856,  ..., -0.4454,  0.3453, -0.2628],\n","         [ 0.3871, -0.9231, -0.1666,  ...,  0.0772, -0.0444,  0.2590],\n","         [-0.0812,  1.4779,  0.8988,  ..., -1.1146,  1.0448,  1.0519]],\n","\n","        [[-0.4362,  0.5773, -0.0062,  ...,  0.9260, -0.1116,  0.4853],\n","         [-0.3355,  0.3770,  0.7200,  ...,  0.0410,  0.2072,  0.0545],\n","         [ 0.0357,  0.1501, -0.6396,  ...,  1.1995, -0.4276,  1.2270],\n","         ...,\n","         [-0.4789,  0.1443,  1.0770,  ...,  0.3193,  0.3184,  0.3362],\n","         [ 0.0622, -0.4742, -0.3382,  ...,  0.8847, -0.1844, -0.0771],\n","         [-0.2194,  0.2591, -0.0956,  ...,  0.3737, -0.1478, -0.2293]],\n","\n","        [[-0.7696,  0.1531, -0.8572,  ..., -0.3858,  0.0534, -0.6434],\n","         [-0.2538,  1.1371,  0.0241,  ..., -0.5719,  0.6910, -0.7004],\n","         [ 1.3673, -0.7895, -0.8872,  ...,  0.2463,  0.0526,  0.5488],\n","         ...,\n","         [ 0.7119, -0.1724,  0.1081,  ..., -0.8122,  0.1016, -0.5708],\n","         [-0.1348,  0.2648,  0.7257,  ..., -0.2262,  0.3651,  0.5955],\n","         [ 0.4330,  0.9720,  0.3746,  ...,  0.4828, -0.4878, -0.0786]],\n","\n","        [[-0.3675,  0.0417,  0.0444,  ..., -0.3165, -0.9441,  0.6468],\n","         [-0.0139,  0.4126,  0.7862,  ...,  0.1945,  0.5425,  0.2139],\n","         [ 0.5893,  0.0330,  1.0036,  ..., -0.0628, -0.2772,  0.7403],\n","         ...,\n","         [ 0.8017,  0.5026, -0.2135,  ...,  0.1897,  0.8163, -0.8402],\n","         [-0.1256,  0.1596, -0.2899,  ...,  0.4217, -0.2899,  0.0264],\n","         [-0.1038, -0.0115, -0.2936,  ...,  0.8732, -0.8528, -0.5979]]],\n","       device='cuda:0')\n","\n","\n","8bit Simu: tensor([[[-5.9261e-01,  2.8091e-01,  1.0133e+00,  ..., -1.8733e-01,\n","           2.9692e-02,  7.6325e-01],\n","         [-4.9105e-01,  6.0987e-01, -2.9606e-03,  ..., -4.8467e-01,\n","           6.3175e-01, -3.2941e-01],\n","         [ 2.9833e-01, -6.7245e-03, -3.6398e-01,  ...,  2.5748e-01,\n","          -8.2788e-01,  4.7420e-01],\n","         ...,\n","         [ 3.3232e-01,  4.7887e-01,  5.4272e-02,  ...,  1.0016e+00,\n","           3.2322e-01, -7.3211e-02],\n","         [ 2.9349e-01,  3.0563e-01, -6.8837e-03,  ...,  4.6803e-01,\n","           2.4927e-01,  3.6242e-01],\n","         [-4.1906e-01,  2.5640e-01, -1.1285e+00,  ..., -1.1550e-01,\n","          -7.1949e-02, -1.5404e-01]],\n","\n","        [[ 1.3781e-01, -1.9370e+00,  2.0087e-01,  ...,  7.3809e-01,\n","           7.6886e-01,  4.3033e-01],\n","         [-1.5533e-01,  6.5420e-01,  3.4026e-01,  ...,  4.0580e-01,\n","          -2.2218e-01, -7.5519e-01],\n","         [ 5.1622e-01,  3.2696e-01, -4.4561e-01,  ...,  3.5097e-01,\n","          -6.9839e-01, -4.6306e-01],\n","         ...,\n","         [-2.0323e-01,  6.4769e-01, -3.8948e-02,  ...,  5.6712e-01,\n","           1.2956e+00, -4.5497e-02],\n","         [ 5.3779e-01, -1.4680e+00, -4.2428e-01,  ...,  7.2733e-01,\n","          -1.1803e-01,  2.2548e-01],\n","         [ 1.3988e-01,  2.2081e-01, -3.6286e-01,  ..., -3.8496e-01,\n","          -2.3678e-01,  6.2480e-01]],\n","\n","        [[ 2.4940e-01, -3.4157e-02,  3.5025e-01,  ..., -1.0596e+00,\n","           8.6665e-02,  1.0424e-02],\n","         [ 8.5830e-01, -1.5284e-01, -8.0544e-02,  ..., -5.0938e-01,\n","           3.5337e-01, -1.6377e-01],\n","         [-7.0850e-01, -3.9858e-01, -3.3288e-01,  ...,  6.9484e-01,\n","           2.4677e-01, -9.2963e-02],\n","         ...,\n","         [ 1.3560e-01,  3.3536e-01, -1.8164e-01,  ...,  2.9085e-02,\n","          -1.0870e+00, -3.1183e-01],\n","         [-3.4939e-01, -1.0087e+00, -6.9315e-01,  ...,  5.4076e-01,\n","           7.5909e-02, -6.2666e-01],\n","         [-8.9094e-01, -1.2764e+00,  4.7541e-01,  ...,  7.8778e-01,\n","          -3.4916e-02,  6.8346e-01]],\n","\n","        [[-4.1954e-01,  1.2548e+00,  1.0186e-01,  ...,  5.8619e-01,\n","          -6.9417e-02,  5.0886e-01],\n","         [ 4.9066e-01, -3.5787e-01,  5.3977e-02,  ...,  9.7308e-01,\n","          -3.0806e-01,  1.1372e-01],\n","         [-5.0131e-02,  3.1357e-01,  2.3857e-01,  ...,  1.3260e-01,\n","          -8.9233e-01, -1.3914e-01],\n","         ...,\n","         [-5.2774e-01,  4.3667e-01,  2.9739e-01,  ..., -8.7842e-01,\n","           7.2085e-01, -3.3410e-01],\n","         [-1.4977e-01,  7.4938e-01,  2.6378e-01,  ...,  1.3459e+00,\n","          -1.2292e-01, -8.0390e-02],\n","         [ 9.7446e-02, -1.8911e-01, -1.1274e-01,  ..., -2.0139e-01,\n","           3.2369e-02, -1.1218e-01]],\n","\n","        [[-7.7156e-01, -7.9361e-02,  9.5444e-01,  ..., -3.0896e-01,\n","          -6.0332e-01, -5.6324e-02],\n","         [ 6.7568e-01, -1.8156e-01,  1.0438e-01,  ...,  4.8385e-01,\n","          -7.7589e-01, -2.7829e-01],\n","         [-2.0769e-01,  2.7214e-01,  2.1489e-01,  ..., -1.1813e-01,\n","           9.3074e-01, -1.6624e-01],\n","         ...,\n","         [ 1.2292e-01, -5.9874e-01, -5.0835e-02,  ...,  7.2011e-01,\n","          -2.3546e-01,  8.2262e-01],\n","         [-1.5809e-04,  6.8708e-01,  4.9703e-01,  ..., -1.4664e+00,\n","           3.0282e-01,  1.0536e+00],\n","         [-4.1247e-01, -6.1890e-01,  4.1352e-03,  ..., -4.6692e-01,\n","           5.2460e-01,  3.9643e-01]]], device='cuda:0')\n","8bit Kern: tensor([[[-5.9261e-01,  2.8091e-01,  1.0133e+00,  ..., -1.8732e-01,\n","           2.9693e-02,  7.6325e-01],\n","         [-4.9105e-01,  6.0987e-01, -2.9607e-03,  ..., -4.8467e-01,\n","           6.3175e-01, -3.2941e-01],\n","         [ 2.9833e-01, -6.7247e-03, -3.6398e-01,  ...,  2.5748e-01,\n","          -8.2789e-01,  4.7420e-01],\n","         ...,\n","         [ 3.3232e-01,  4.7887e-01,  5.4272e-02,  ...,  1.0016e+00,\n","           3.2322e-01, -7.3211e-02],\n","         [ 2.9349e-01,  3.0563e-01, -6.8837e-03,  ...,  4.6803e-01,\n","           2.4927e-01,  3.6242e-01],\n","         [-4.1906e-01,  2.5640e-01, -1.1285e+00,  ..., -1.1550e-01,\n","          -7.1949e-02, -1.5404e-01]],\n","\n","        [[ 1.3781e-01, -1.9370e+00,  2.0087e-01,  ...,  7.3809e-01,\n","           7.6886e-01,  4.3033e-01],\n","         [-1.5533e-01,  6.5420e-01,  3.4026e-01,  ...,  4.0580e-01,\n","          -2.2218e-01, -7.5519e-01],\n","         [ 5.1622e-01,  3.2697e-01, -4.4561e-01,  ...,  3.5097e-01,\n","          -6.9839e-01, -4.6306e-01],\n","         ...,\n","         [-2.0323e-01,  6.4769e-01, -3.8948e-02,  ...,  5.6712e-01,\n","           1.2956e+00, -4.5497e-02],\n","         [ 5.3779e-01, -1.4680e+00, -4.2428e-01,  ...,  7.2733e-01,\n","          -1.1802e-01,  2.2548e-01],\n","         [ 1.3988e-01,  2.2081e-01, -3.6286e-01,  ..., -3.8496e-01,\n","          -2.3678e-01,  6.2480e-01]],\n","\n","        [[ 2.4940e-01, -3.4158e-02,  3.5025e-01,  ..., -1.0596e+00,\n","           8.6666e-02,  1.0424e-02],\n","         [ 8.5830e-01, -1.5284e-01, -8.0545e-02,  ..., -5.0937e-01,\n","           3.5337e-01, -1.6377e-01],\n","         [-7.0850e-01, -3.9858e-01, -3.3288e-01,  ...,  6.9484e-01,\n","           2.4677e-01, -9.2963e-02],\n","         ...,\n","         [ 1.3560e-01,  3.3536e-01, -1.8164e-01,  ...,  2.9084e-02,\n","          -1.0870e+00, -3.1183e-01],\n","         [-3.4939e-01, -1.0087e+00, -6.9315e-01,  ...,  5.4076e-01,\n","           7.5909e-02, -6.2666e-01],\n","         [-8.9094e-01, -1.2764e+00,  4.7541e-01,  ...,  7.8778e-01,\n","          -3.4916e-02,  6.8346e-01]],\n","\n","        [[-4.1954e-01,  1.2548e+00,  1.0186e-01,  ...,  5.8619e-01,\n","          -6.9417e-02,  5.0886e-01],\n","         [ 4.9066e-01, -3.5787e-01,  5.3977e-02,  ...,  9.7308e-01,\n","          -3.0806e-01,  1.1372e-01],\n","         [-5.0131e-02,  3.1357e-01,  2.3857e-01,  ...,  1.3260e-01,\n","          -8.9233e-01, -1.3914e-01],\n","         ...,\n","         [-5.2774e-01,  4.3667e-01,  2.9739e-01,  ..., -8.7842e-01,\n","           7.2086e-01, -3.3410e-01],\n","         [-1.4977e-01,  7.4938e-01,  2.6378e-01,  ...,  1.3459e+00,\n","          -1.2292e-01, -8.0389e-02],\n","         [ 9.7446e-02, -1.8911e-01, -1.1274e-01,  ..., -2.0139e-01,\n","           3.2369e-02, -1.1218e-01]],\n","\n","        [[-7.7156e-01, -7.9361e-02,  9.5444e-01,  ..., -3.0896e-01,\n","          -6.0332e-01, -5.6325e-02],\n","         [ 6.7568e-01, -1.8156e-01,  1.0438e-01,  ...,  4.8385e-01,\n","          -7.7589e-01, -2.7828e-01],\n","         [-2.0769e-01,  2.7214e-01,  2.1489e-01,  ..., -1.1813e-01,\n","           9.3074e-01, -1.6624e-01],\n","         ...,\n","         [ 1.2292e-01, -5.9874e-01, -5.0835e-02,  ...,  7.2011e-01,\n","          -2.3546e-01,  8.2262e-01],\n","         [-1.5771e-04,  6.8708e-01,  4.9703e-01,  ..., -1.4664e+00,\n","           3.0282e-01,  1.0536e+00],\n","         [-4.1247e-01, -6.1890e-01,  4.1343e-03,  ..., -4.6692e-01,\n","           5.2460e-01,  3.9643e-01]]], device='cuda:0')\n"]}]},{"cell_type":"code","source":["!git lfs install && git clone https://huggingface.co/BelleGroup/BELLE-LLAMA-7B-2M-gptq"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MlnUTqWIS8Si","executionInfo":{"status":"ok","timestamp":1680786132596,"user_tz":-480,"elapsed":189755,"user":{"displayName":"Jeromy Gladden","userId":"11403350924762830733"}},"outputId":"217c3a7b-9fa5-4c25-98e5-56a720c7810e"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Updated git hooks.\n","Git LFS initialized.\n","Cloning into 'BELLE-LLAMA-7B-2M-gptq'...\n","remote: Enumerating objects: 34, done.\u001b[K\n","remote: Counting objects: 100% (34/34), done.\u001b[K\n","remote: Compressing objects: 100% (32/32), done.\u001b[K\n","remote: Total 34 (delta 12), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (34/34), 10.40 KiB | 1.30 MiB/s, done.\n","Filtering content: 100% (3/3), 6.48 GiB | 35.28 MiB/s, done.\n","Encountered 1 file(s) that may not have been copied correctly on Windows:\n","\tllama7b-2m-8bit-128g.pt\n","\n","See: `git lfs help smudge` for more details.\n"]}]},{"cell_type":"code","source":["!ls BELLE-LLAMA-7B-2M-gptq"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-bbhBh8lTK-K","executionInfo":{"status":"ok","timestamp":1680786324610,"user_tz":-480,"elapsed":879,"user":{"displayName":"Jeromy Gladden","userId":"11403350924762830733"}},"outputId":"75246e86-6a35-4ebe-c3e7-e6b99ca890e2"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["config.json\t\tllama7b-2m-4bit-128g.pt  special_tokens_map.json\n","generation_config.json\tllama7b-2m-8bit-128g.pt  tokenizer_config.json\n","LICENSE\t\t\tREADME.md\t\t tokenizer.model\n"]}]},{"cell_type":"code","source":["%cd BELLE/gptq"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qquY__cLrowZ","executionInfo":{"status":"ok","timestamp":1680786516485,"user_tz":-480,"elapsed":11,"user":{"displayName":"Jeromy Gladden","userId":"11403350924762830733"}},"outputId":"6f36fbc9-e70c-4dbb-f713-ee34d224b929"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/BELLE/gptq\n"]}]},{"cell_type":"code","source":["import time\n","import transformers\n","import torch\n","import torch.nn as nn\n","\n","from gptq import *\n","from modelutils import *\n","from quant import *\n","\n","from transformers import AutoTokenizer\n","\n","DEV = torch.device('cuda:0')\n","\n","def get_llama(model):\n","    import torch\n","    def skip(*args, **kwargs):\n","        pass\n","    torch.nn.init.kaiming_uniform_ = skip\n","    torch.nn.init.uniform_ = skip\n","    torch.nn.init.normal_ = skip\n","    from transformers import LlamaForCausalLM\n","    model = LlamaForCausalLM.from_pretrained(model, torch_dtype='auto')\n","    model.seqlen = 2048\n","    return model\n","\n","def load_quant(model, checkpoint, wbits, groupsize):\n","    from transformers import LlamaConfig, LlamaForCausalLM \n","    config = LlamaConfig.from_pretrained(model)\n","    def noop(*args, **kwargs):\n","        pass\n","    torch.nn.init.kaiming_uniform_ = noop \n","    torch.nn.init.uniform_ = noop \n","    torch.nn.init.normal_ = noop \n","\n","    torch.set_default_dtype(torch.half)\n","    transformers.modeling_utils._init_weights = False\n","    torch.set_default_dtype(torch.half)\n","    model = LlamaForCausalLM(config)\n","    torch.set_default_dtype(torch.float)\n","    model = model.eval()\n","    layers = find_layers(model)\n","    for name in ['lm_head']:\n","        if name in layers:\n","            del layers[name]\n","    make_quant(model, layers, wbits, groupsize)\n","\n","    print('Loading model ...')\n","    if checkpoint.endswith('.safetensors'):\n","        from safetensors.torch import load_file as safe_load\n","        model.load_state_dict(safe_load(checkpoint))\n","    else:\n","        model.load_state_dict(torch.load(checkpoint))\n","    model.seqlen = 2048\n","    print('Done.')\n","\n","    return model\n","\n","\n","class args:\n","    model = \"BELLE-LLAMA-7B-2M-gptq\"\n","    wbits = 4\n","    groupsize = 128\n","    load = \"BELLE-LLAMA-7B-2M-gptq/llama7b-2m-4bit-128g.pt\"\n","    text = None\n","    min_length = 10\n","    max_length = 1024\n","    top_p = 0.95\n","    temperature = 0.7\n","\n","\n","    \n","    \n","if type(args.load) is not str:\n","    args.load = args.load.as_posix()\n","\n","\n","\n","\n","model = load_quant(args.model, args.load, args.wbits, args.groupsize)\n","\n","\n","model.to(DEV)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"moeeqmowTXX7","executionInfo":{"status":"ok","timestamp":1680786553799,"user_tz":-480,"elapsed":31877,"user":{"displayName":"Jeromy Gladden","userId":"11403350924762830733"}},"outputId":"3277ec0e-cd00-410f-c89b-1a78b3582803"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model ...\n","Done.\n"]},{"output_type":"execute_result","data":{"text/plain":["LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n","    (layers): ModuleList(\n","      (0-31): 32 x LlamaDecoderLayer(\n","        (self_attn): LlamaAttention(\n","          (q_proj): QuantLinear()\n","          (k_proj): QuantLinear()\n","          (v_proj): QuantLinear()\n","          (o_proj): QuantLinear()\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): QuantLinear()\n","          (down_proj): QuantLinear()\n","          (up_proj): QuantLinear()\n","          (act_fn): SiLUActivation()\n","        )\n","        (input_layernorm): LlamaRMSNorm()\n","        (post_attention_layernorm): LlamaRMSNorm()\n","      )\n","    )\n","    (norm): LlamaRMSNorm()\n","  )\n","  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",")"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["\n","from transformers import LlamaForCausalLM, LlamaTokenizer\n","tokenizer = LlamaTokenizer.from_pretrained(\"BelleGroup/BELLE-LLAMA-7B-0.6M\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["848961612085493ab0b342e73976f094","cd4198f0355c487ba0a37f8c0e9894cf","088a7fc461b549aa9ec9648e88cb9cb1","e77bec041ca242ab889de11ff7fc752a","0a5175a6beba4f43aa65d5ad7e8415ff","bbf7466a5cd348b798936e1539127a93","f6975c606d0d4992b4adfea3621dfb5f","ebedb3a838b24a9098ff0ad6c02f6c5b","e3f11dab8b4741d1ad4af2e2c6e646af","a5d4eab68ee74cb7857a274a2505ab7a","e0c42551bb6148b7a10c5757c479fc89","9a7adee865e64b9b97c509993262435c","b2dbf4a90f444bf38f8fdca5ac1f0f6a","1310ad76a0674568a167c183ebfeb12e","35106bfc17d1416db4ae44d4235a2797","2e2b57ec74f84fbeaa9fbdf208063bf2","0e13b07444d9461fb61875dcb4c28e56","4f70f67a47cf488ca18db2f8abf60c56","a1c782f40e754e5da69595fb17c9d5db","91e750624fd0485bb4e3ce0b7a1c0a68","68ccd5606bea46008eb0b5666de2627f","f10803b9a2f2405ea3837b6512297c29","c369bced4bcd4a42b6f6938d4ff50767","81759085d51c46eda92930ddcfa536c9","0c7c60b42b5d4eadb7a409eaf1fbe000","39094cb45c8c46cfa0d2e80209c0ce4f","85f5c3a6c6c14cb9858ac840c7a98014","95c739369d79481f902992045358895f","d060bf8dc4ea4469bc3f9a8ac24dac10","c451fbd57a4e42789b45f39568eefabe","a0c5a11894114dd8b29f1d6aaf8025ca","a729419943dc4cff91204fdfaf44f142","a7211eec9aae4e5fa1b68060b6d11661"]},"id":"bnAa2VOxTaoa","executionInfo":{"status":"ok","timestamp":1680786579627,"user_tz":-480,"elapsed":823,"user":{"displayName":"Jeromy Gladden","userId":"11403350924762830733"}},"outputId":"ebf16e57-3014-4ee6-d9b7-eb16622996fd"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"848961612085493ab0b342e73976f094"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a7adee865e64b9b97c509993262435c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c369bced4bcd4a42b6f6938d4ff50767"}},"metadata":{}}]},{"cell_type":"code","source":["def infer_text_gen(text):\n","    inputs = f'Human: {text} \\n\\nAssistant:'\n","    input_ids = tokenizer.encode(inputs, return_tensors=\"pt\").to(DEV)\n","\n","    with torch.no_grad():\n","        generated_ids = model.generate(\n","            input_ids,\n","            do_sample=True,\n","            min_length=args.min_length,\n","            max_length=2048,\n","            top_p=args.top_p,\n","            temperature=args.temperature,\n","        )\n","\n","    decode_text = tokenizer.decode([el.item() for el in generated_ids[0]])\n","    decode_text = decode_text[len(inputs):]\n","    decode_text = decode_text.replace(\"\",\"\")\n","    return decode_text"],"metadata":{"id":"iCsiZXM0TeU5","executionInfo":{"status":"ok","timestamp":1680788749834,"user_tz":-480,"elapsed":1196,"user":{"displayName":"Jeromy Gladden","userId":"11403350924762830733"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["print(infer_text_gen(\"请帮我写一首诗歌。\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"feFZMHNRTg6a","executionInfo":{"status":"ok","timestamp":1680788773451,"user_tz":-480,"elapsed":21232,"user":{"displayName":"Jeromy Gladden","userId":"11403350924762830733"}},"outputId":"6716262c-0de9-4238-d404-9d9558ebbe33"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["ant:当我闭上双眼，\n","眼前是漆黑的夜，\n","却又感受到微弱的光，\n","那就是心中的希望。\n","\n","当我感到无助和绝望，\n","生活似乎在我的脚下沉淀，\n","却依然感受到希望的微笑，\n","那就是心中的信念。\n","\n","当我陷入沉思和自我怀疑，\n","却仍然听到内心的声音，\n","那就是心中的勇气和坚定，\n","让我坚定前行的信念。\n","\n","虽然路途漫长，\n","但我会牢记心中的希望和信念，\n","因为它们是我前行的力量和方向，\n","让我在黑暗中看到光明。\n"]}]},{"cell_type":"code","source":["import json\n","from tqdm import tqdm\n","with open('../../drive/MyDrive/belle/new/train.json', 'r', encoding='utf-8') as read_file, open('../../drive/MyDrive/belle/new/train_summary.json', 'w', encoding='utf-8') as write_file:\n","    read_lines = read_file.readlines()\n","    total_number = len(read_lines)\n","    for index in tqdm(range(total_number)):\n","      line = read_lines[index]\n","      total_message = json.loads(line)\n","      content = total_message['content']\n","      title = total_message['title']\n","      url = total_message['url']\n","      pub_time = total_message['pub_time']\n","      entities = total_message['entities']\n","\n","      content_list = []\n","      while len(content) >= 20:\n","          content_list.append(content[:768])\n","          content = content[768:]\n","      restext_list = []\n","      for content_index, content_item in enumerate(content_list):\n","          response = '下面是一篇文章的一部分，请你帮我将他缩写为长度在112字左右的摘要。内容如下：' + content_item\n","          restext = infer_text_gen(response)\n","          restext_list.append(restext)\n","      write_file.write(\n","          json.dumps({\"pub_time\": pub_time, \"title\": title, \"id\": index, \"url\": url, 'content': ' '.join(restext_list), \"entities\": entities}, ensure_ascii=False) + '\\n')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"id":"T91hFEKYudu6","executionInfo":{"status":"error","timestamp":1680791349149,"user_tz":-480,"elapsed":2529224,"user":{"displayName":"Jeromy Gladden","userId":"11403350924762830733"}},"outputId":"26f30998-dca3-4e61-b5e4-2af56a154f1e"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 12/4700 [41:54<272:51:06, 209.53s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-caa573de08b2>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mcontent_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_item\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'下面是一篇文章的一部分，请你帮我将他缩写为长度在112字左右的摘要。内容如下：'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcontent_item\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m           \u001b[0mrestext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_text_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m           \u001b[0mrestext_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       write_file.write(\n","\u001b[0;32m<ipython-input-14-77c6a080f2a2>\u001b[0m in \u001b[0;36minfer_text_gen\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         generated_ids = model.generate(\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1486\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2558\u001b[0m             \u001b[0;31m# sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2559\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m             \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m             \u001b[0;31m# finished sentences should have their next token be a padding token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"1N93isrFfLNkGiU4Xvh4hOhLpucJzzZaY","authorship_tag":"ABX9TyMYXWjcmguNhTNWJe7BicQs"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"848961612085493ab0b342e73976f094":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cd4198f0355c487ba0a37f8c0e9894cf","IPY_MODEL_088a7fc461b549aa9ec9648e88cb9cb1","IPY_MODEL_e77bec041ca242ab889de11ff7fc752a"],"layout":"IPY_MODEL_0a5175a6beba4f43aa65d5ad7e8415ff"}},"cd4198f0355c487ba0a37f8c0e9894cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbf7466a5cd348b798936e1539127a93","placeholder":"​","style":"IPY_MODEL_f6975c606d0d4992b4adfea3621dfb5f","value":"Downloading tokenizer.model: 100%"}},"088a7fc461b549aa9ec9648e88cb9cb1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebedb3a838b24a9098ff0ad6c02f6c5b","max":499723,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e3f11dab8b4741d1ad4af2e2c6e646af","value":499723}},"e77bec041ca242ab889de11ff7fc752a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5d4eab68ee74cb7857a274a2505ab7a","placeholder":"​","style":"IPY_MODEL_e0c42551bb6148b7a10c5757c479fc89","value":" 500k/500k [00:00&lt;00:00, 13.8MB/s]"}},"0a5175a6beba4f43aa65d5ad7e8415ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbf7466a5cd348b798936e1539127a93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6975c606d0d4992b4adfea3621dfb5f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebedb3a838b24a9098ff0ad6c02f6c5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3f11dab8b4741d1ad4af2e2c6e646af":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a5d4eab68ee74cb7857a274a2505ab7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0c42551bb6148b7a10c5757c479fc89":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a7adee865e64b9b97c509993262435c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b2dbf4a90f444bf38f8fdca5ac1f0f6a","IPY_MODEL_1310ad76a0674568a167c183ebfeb12e","IPY_MODEL_35106bfc17d1416db4ae44d4235a2797"],"layout":"IPY_MODEL_2e2b57ec74f84fbeaa9fbdf208063bf2"}},"b2dbf4a90f444bf38f8fdca5ac1f0f6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e13b07444d9461fb61875dcb4c28e56","placeholder":"​","style":"IPY_MODEL_4f70f67a47cf488ca18db2f8abf60c56","value":"Downloading (…)cial_tokens_map.json: 100%"}},"1310ad76a0674568a167c183ebfeb12e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1c782f40e754e5da69595fb17c9d5db","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_91e750624fd0485bb4e3ce0b7a1c0a68","value":2}},"35106bfc17d1416db4ae44d4235a2797":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68ccd5606bea46008eb0b5666de2627f","placeholder":"​","style":"IPY_MODEL_f10803b9a2f2405ea3837b6512297c29","value":" 2.00/2.00 [00:00&lt;00:00, 88.4B/s]"}},"2e2b57ec74f84fbeaa9fbdf208063bf2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e13b07444d9461fb61875dcb4c28e56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f70f67a47cf488ca18db2f8abf60c56":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a1c782f40e754e5da69595fb17c9d5db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91e750624fd0485bb4e3ce0b7a1c0a68":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68ccd5606bea46008eb0b5666de2627f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f10803b9a2f2405ea3837b6512297c29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c369bced4bcd4a42b6f6938d4ff50767":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_81759085d51c46eda92930ddcfa536c9","IPY_MODEL_0c7c60b42b5d4eadb7a409eaf1fbe000","IPY_MODEL_39094cb45c8c46cfa0d2e80209c0ce4f"],"layout":"IPY_MODEL_85f5c3a6c6c14cb9858ac840c7a98014"}},"81759085d51c46eda92930ddcfa536c9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95c739369d79481f902992045358895f","placeholder":"​","style":"IPY_MODEL_d060bf8dc4ea4469bc3f9a8ac24dac10","value":"Downloading (…)okenizer_config.json: 100%"}},"0c7c60b42b5d4eadb7a409eaf1fbe000":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c451fbd57a4e42789b45f39568eefabe","max":141,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a0c5a11894114dd8b29f1d6aaf8025ca","value":141}},"39094cb45c8c46cfa0d2e80209c0ce4f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a729419943dc4cff91204fdfaf44f142","placeholder":"​","style":"IPY_MODEL_a7211eec9aae4e5fa1b68060b6d11661","value":" 141/141 [00:00&lt;00:00, 7.79kB/s]"}},"85f5c3a6c6c14cb9858ac840c7a98014":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95c739369d79481f902992045358895f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d060bf8dc4ea4469bc3f9a8ac24dac10":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c451fbd57a4e42789b45f39568eefabe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0c5a11894114dd8b29f1d6aaf8025ca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a729419943dc4cff91204fdfaf44f142":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7211eec9aae4e5fa1b68060b6d11661":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}